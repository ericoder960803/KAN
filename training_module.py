def train_with_val_safe(models_dict, train_loader, test_loader, epochs=30):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    save_dir = "checkpoints"
    os.makedirs(save_dir, exist_ok=True)
    
    all_history = {}

    for name, model in models_dict.items():
        print(f"\n é¸æ‰‹å…¥å ´: {name}")
        model.to(device)
        
        # å„ªåŒ–å™¨å»ºè­°ç”¨ AdamW æ­é…é‡é‡è¡°æ¸›
        optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
        scaler = GradScaler()
        
        start_epoch = 0
        best_test_acc = 0.0
        history = {'train_acc': [], 'test_acc': [], 'loss': [], 'l1': []}
        
        checkpoint_path = os.path.join(save_dir, f"{name}_resume.pth")
        
        # å˜—è©¦è®€å–çºŒå‚³å­˜æª”
        if os.path.exists(checkpoint_path):
            try:
                ckpt = torch.load(checkpoint_path)
                model.load_state_dict(ckpt['model_state_dict'])
                optimizer.load_state_dict(ckpt['optimizer_state_dict'])
                scheduler.load_state_dict(ckpt['scheduler_state_dict'])
                start_epoch = ckpt['epoch'] + 1
                best_test_acc = ckpt.get('best_test_acc', 0.0)
                history = ckpt.get('history', history)
                print(f"åµæ¸¬åˆ°å­˜æª”ï¼Œå¾ Epoch {start_epoch} æ¢å¾©ã€‚ç›®å‰æœ€ä½³æ¸¬è©¦ Acc: {best_test_acc:.4f}")
            except Exception as e:
                print(f" è®€å–å­˜æª”å¤±æ•—ï¼Œå°‡é‡æ–°é–‹å§‹ã€‚åŸå› : {e}")

        # å®šç¾©çµ±ä¸€ä¿å­˜é‚è¼¯
        def save_now(current_epoch, is_best=False):
            state = {
                'epoch': current_epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_test_acc': best_test_acc,
                'history': history
            }
            torch.save(state, checkpoint_path)
            if is_best:
                torch.save(model.state_dict(), os.path.join(save_dir, f"{name}_best_real.pth"))

        try:
            for epoch in range(start_epoch, epochs):
                # --- è¨“ç·´éšæ®µ ---
                model.train()
                train_correct, train_total, ep_loss, ep_l1 = 0, 0, 0, 0
                pbar = tqdm.tqdm(train_loader, desc=f"{name} Ep {epoch+1}")
                
                for imgs, lbls in pbar:
                    imgs, lbls = imgs.to(device), lbls.to(device)
                    optimizer.zero_grad()
                    with autocast():
                        out = model(imgs)
                        ce_loss = F.cross_entropy(out, lbls)
                        # L1 æ­£å‰‡åŒ– (å¦‚æœä½ éœ€è¦ä¿æŒæ¨¡å‹ç¨€ç–)
                        l1_val = sum(p.abs().sum() for p in model.parameters())
                        total_loss = ce_loss + 1e-5 * l1_val
                    
                    scaler.scale(total_loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                    
                    train_correct += (out.argmax(1) == lbls).sum().item()
                    train_total += lbls.size(0)
                    ep_loss += ce_loss.item()
                    ep_l1 += l1_val.item()
                    pbar.set_postfix({"TrainAcc": f"{train_correct/train_total:.4f}"})

                # --- é©—è­‰éšæ®µ (for early stop ) ---
                model.eval()
                test_correct, test_total = 0, 0
                with torch.no_grad():
                    for t_imgs, t_lbls in test_loader:
                        t_imgs, t_lbls = t_imgs.to(device), t_lbls.to(device)
                        t_out = model(t_imgs)
                        test_correct += (t_out.argmax(1) == t_lbls).sum().item()
                        test_total += t_lbls.size(0)
                
                cur_test_acc = test_correct / test_total
                scheduler.step()
                
                # ç´€éŒ„æ­·å²
                history['train_acc'].append(train_correct / train_total)
                history['test_acc'].append(cur_test_acc)
                history['loss'].append(ep_loss / len(train_loader))
                history['l1'].append(ep_l1 / len(train_loader))
                # for early stop 
                is_best = cur_test_acc > best_test_acc
                if is_best:
                    best_test_acc = cur_test_acc
                
                # forced saved 
                save_now(epoch, is_best=is_best)
                print(f"ğŸ“‰ Ep {epoch+1} å®Œæˆ! Train Acc: {history['train_acc'][-1]:.4f} | Test Acc: {cur_test_acc:.4f}")

        except KeyboardInterrupt:
            print("\nğŸ›‘ åµæ¸¬åˆ°æ‰‹å‹•ä¸­æ–·ï¼æ­£åœ¨ç·Šæ€¥ä¿å­˜ç›®å‰é€²åº¦...")
            save_now(epoch) 
            print("ğŸ’¾ ç·Šæ€¥å­˜æª”æˆåŠŸï¼ä¸‹æ¬¡åŸ·è¡Œå°‡è‡ªå‹•æ¥çºŒã€‚")
            break
        
        all_history[name] = history
    return all_history
